mutate(text_id = parse_number(ids) %>%
str_pad(3,"left","0") %>%
paste0(doc_id,.)) %>%
select(doc_id,text_id,text)
HOC <-
HOC%>%
# individual rows for each text
tidyr::separate_rows(text, sep = "@..@") %>%
# remove empty strings
filter(!(text == "")) %>%
# extract ids
mutate(ids = str_extract(text, "#\\d+#")) %>%
# clean text, doc_id and text_id
mutate(text = str_remove(text,ids) %>% str_squish) %>%
mutate(doc_id = str_extract(doc_id, "^..")) %>%
mutate(text_id = parse_number(ids) %>%
str_pad(3,"left","0") %>%
paste0(doc_id,.)) %>%
select(doc_id,text_id,text)
HOC <- readtext(here("data","HSC","HYS ALL DIR"))
HOC <- readtext("/Users/timmarchand/Documents/2022_project/data/HSC/HYS ALL DIR")
HOC <-
HOC%>%
# individual rows for each text
tidyr::separate_rows(text, sep = "@..@") %>%
# remove empty strings
filter(!(text == "")) %>%
# extract ids
mutate(ids = str_extract(text, "#\\d+#")) %>%
# clean text, doc_id and text_id
mutate(text = str_remove(text,ids) %>% str_squish) %>%
mutate(doc_id = str_extract(doc_id, "^..")) %>%
mutate(text_id = parse_number(ids) %>%
str_pad(3,"left","0") %>%
paste0(doc_id,.)) %>%
select(doc_id,text_id,text)
HOC <-
HOC%>%
# individual rows for each text
tidyr::separate_rows(text, sep = "@..@") %>%
# remove empty strings
filter(!(text == "")) %>%
# extract ids
mutate(ids = str_extract(text, "#\\d+#")) %>%
# clean text, doc_id and text_id
mutate(text = str_remove(text,ids) %>% str_squish) %>%
mutate(doc_id = str_extract(doc_id, "^..")) %>%
mutate(text_id = readr::parse_number(ids) %>%
str_pad(3,"left","0") %>%
paste0(doc_id,.)) %>%
select(doc_id,text_id,text)
HOC
HOC %>% head(500) %>%
mutate(text = multi_replace_all(text, repl_US))
HOC_UK_spell <- HOC  %>% head(500) %>%
mutate(text = multi_replace_all(text, repl_US))
HOC_UK_spell %>%
filter(str_detect(text, "\\{"))
HOC_UK_spell %>%
filter(str_detect(text, "\\{")) %>%
pull(text) %>%
str_extract_all("\\w+\\{.+?\\}")
HOC_UK_spell <- HOC  %>% head(5000) %>%
mutate(text = multi_replace_all(text, repl_US))
HOC_UK_spell %>%
filter(str_detect(text, "\\{")) %>%
pull(text) %>%
str_extract_all("\\w+\\{.+?\\}")
HOC_UK_spell %>%
filter(str_detect(text, "\\{")) %>%
pull(text) %>%
str_extract_all("\\w+\\{.+?\\}") %>%
flatten
HOC_UK_spell %>%
filter(str_detect(text, "\\{")) %>%
pull(text) %>%
str_extract_all("\\w+\\{.+?\\}") %>%
unlist
HOC_UK_spell %>% mutate(wc = str_count(text, " "))
HOC_UK_spell %>% mutate(wc = str_count(text, " ")) %>%
summarise(sum(wc))
HOC_UK_spell %>% mutate(wc = str_count(text, " ")) %>%
summarise(sum(wc)) %>% pull %>%  311/ .
HOC_UK_spell %>% mutate(wc = str_count(text, " ")) %>%
summarise(wc = sum(wc)) %>% pull(wc) %>%  311/ .
HOC_UK_spell %>% mutate(wc = str_count(text, " ")) %>%
summarise(wc = sum(wc)) %>% mutate(percent = 311 / wc)
HOC_UK_spell %>% mutate(wc = str_count(text, " ")) %>%
summarise(wc = sum(wc)) %>% mutate(percent = 311*100 / wc)
JOC
JOC_UK_spell <- JOC %>% sample_frac(0.2)%>%
mutate(text = multi_replace_all(text, repl_US))
JOC_UK_spell %>%
mutate(wc = str_count(text, " "),
change = str_count(text, "\\{")) %>%
summarise(prop = 100*sum(change) / sum(wc) )
JOC_UK_spell %>%
mutate(wc = str_count(text, " "),
change = str_count(text, "\\{"))
JOC_UK_spell %>%
mutate(wc = str_count(text, " "),
change = str_count(text, "\\{")) %>%
summarise(prop = 100*sum(change) / sum(wc) )
JOC_UK_spell %>%
pull(text) %>%
str_extract_all("\\w+\\{.+?\\}") %>%
unlist
repl_UK <-
repl_US %>%
rename(token = new, new = token)
JOC_US_spell <- JOC %>% sample_frac(0.2)%>%
mutate(text = multi_replace_all(text, repl_UK))
repl_UK
repl_US %>%
mutate(token = str_remove(token, "\\{.*$"))
repl_US %>%
mutate(new = str_remove(new, "\\{.*$"))
repl_UK <-
repl_US %>%
mutate(new = str_remove(new, "\\{.*$"),
token = str_c(token, "\\{replaced=",new,":UK}}")) %>%
select(token = new, new = token)
repl_UK
repl_UK <-
repl_US %>%
mutate(new = str_remove(new, "\\{.*$"),
token = str_c(token, "{{replaced=",new,":UK}}")) %>%
select(token = new, new = token)
repl_UK
JOC_US_spell <- JOC %>% sample_frac(0.2)%>%
mutate(text = multi_replace_all(text, repl_UK))
JOC_US_spell %>%
mutate(wc = str_count(text, " "),
change = str_count(text, "\\{")) %>%
summarise(prop = 100*sum(change) / sum(wc) )
HOC_US_spell <- HOC  %>% sample_frac(0.2) %>%
mutate(text = multi_replace_all(text, repl_UK))
HOC_US_spell %>% mutate(wc = str_count(text, " ")) %>%
mutate(wc = str_count(text, " "),
change = str_count(text, "\\{")) %>%
summarise(prop = 100*sum(change) / sum(wc) )
HOC_UK_spell <- HOC  %>% sample_frac(0.2) %>%
mutate(text = multi_replace_all(text, repl_US))
HOC_UK_spell %>% mutate(wc = str_count(text, " ")) %>%
mutate(wc = str_count(text, " "),
change = str_count(text, "\\{")) %>%
summarise(prop = 100*sum(change) / sum(wc) )
JOC_UK_spell %>%
mutate(wc = str_count(text, " "),
change = str_count(text, "\\{")) %>%
summarise(JOC_US_prop = 100*sum(change) / sum(wc) )
### US UK spelling
repl_US <- readr::read_csv("https://docs.google.com/spreadsheets/d/e/2PACX-1vQ8rUWjf-MnofifXO0EPLmLm_u_uTR5psMl9Ne34wTHZwzBi1fAyxaZqxAqWqR8HkT_eL6Q4pSR_L39/pub?gid=0&single=true&output=csv")
JOC <- readr::read_csv("/Users/timmarchand/Documents/2022_project/data/JSC/corrected_text.csv")
JOC_UK_spell <- JOC %>%
mutate(text = multi_replace_all(text, repl_US))
JOC_US_spell <- JOC %>%
mutate(text = multi_replace_all(text, repl_UK))
repl_UK <-
repl_US %>%
mutate(new = str_remove(new, "\\{.*$"),
token = str_c(token, "{{replaced=",new,":UK}}")) %>%
select(token = new, new = token)
JOC_US <-
JOC_UK_spell %>%
mutate(wc = str_count(text, " "),
change = str_count(text, "\\{")) %>%
summarise(prop = 100*sum(change) / sum(wc) ) %>%
pull(prop)
JOC_UK <-
JOC_US_spell %>%
mutate(wc = str_count(text, " "),
change = str_count(text, "\\{")) %>%
summarise(prop = 100*sum(change) / sum(wc) ) %>%
pull(prop)
JOC_UK_spell %>%
pull(text) %>%
str_extract_all("\\w+\\{.+?\\}") %>%
unlist
HOC <- readtext("/Users/timmarchand/Documents/2022_project/data/HSC/HYS ALL DIR")
HOC <-
HOC%>%
# individual rows for each text
tidyr::separate_rows(text, sep = "@..@") %>%
# remove empty strings
filter(!(text == "")) %>%
# extract ids
mutate(ids = str_extract(text, "#\\d+#")) %>%
# clean text, doc_id and text_id
mutate(text = str_remove(text,ids) %>% str_squish) %>%
mutate(doc_id = str_extract(doc_id, "^..")) %>%
mutate(text_id = readr::parse_number(ids) %>%
str_pad(3,"left","0") %>%
paste0(doc_id,.)) %>%
select(doc_id,text_id,text)
HOC_UK_spell <- HOC  %>%
mutate(text = multi_replace_all(text, repl_US))
HOC_US_spell <- HOC  %>%
mutate(text = multi_replace_all(text, repl_UK))
HOC_UK <-
HOC_US_spell %>% mutate(wc = str_count(text, " ")) %>%
mutate(wc = str_count(text, " "),
change = str_count(text, "\\{")) %>%
summarise(prop = 100*sum(change) / sum(wc) ) %>%
pull(prop)
HOC_US <-
HOC_UK_spell %>% mutate(wc = str_count(text, " ")) %>%
mutate(wc = str_count(text, " "),
change = str_count(text, "\\{")) %>%
summarise(prop = 100*sum(change) / sum(wc) ) %>%
pull(prop)
tibble(HOC_UK, HOC_US, JOC_UK, JOC_US)
JOC_UK_spell
type = str_split_1(text) %>% unique %>% length)
JOC_UK_spell %>%
summarise(text = str_c(text, collapse = " ") %>%
mutate(tokens = str_count(" "),
type = str_split_1(text) %>% unique %>% length))
JOC_UK_spell %>%
summarise(text = str_c(text, collapse = " ")) %>%
mutate(tokens = str_count(" "),
type = str_split_1(text) %>% unique %>% length))
JOC_UK_spell %>%
summarise(text = str_c(text, collapse = " ")) %>%
mutate(tokens = str_count(" "),
type = str_split_1(text) %>% unique %>% length)
JOC_UK_spell %>%
summarise(text = str_c(text, collapse = " ")) %>%
mutate(tokens = str_count(" "),
type = str_split_1(text, " ") %>% unique %>% length)
JOC_UK_spell %>%
summarise(text = str_c(text, collapse = " ")) %>%
mutate(tokens = str_count(text, " "),
type = str_split_1(text, " ") %>% unique %>% length)
JOC_UK_spell %>%
summarise(text = str_c(text, collapse = " ")) %>%
mutate(tokens = str_count(text, " "),
type = str_split_1(text, " ") %>% unique %>% length,
ttr = type / tokens)
JOC %>%
summarise(text = str_c(text, collapse = " ")) %>%
mutate(tokens = str_count(text, " "),
type = str_split_1(text, " ") %>% unique %>% length,
ttr = type / tokens)
JOC_US_spell  %>%
summarise(text = str_c(text, collapse = " ")) %>%
mutate(tokens = str_count(text, " "),
type = str_split_1(text, " ") %>% unique %>% length,
ttr = type / tokens)
JOC_US_spell
JOC_US_spell  %>%
mutate(text = str_remove(text, "\\{.+?\\}"))
JOC_US_spell  %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
mutate(tokens = str_count(text, " "),
type = str_split_1(text, " ") %>% unique %>% length,
ttr = type / tokens)
JOC_UK_spell %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
mutate(tokens = str_count(text, " "),
type = str_split_1(text, " ") %>% unique %>% length,
ttr = type / tokens)
JOC %>%
summarise(text = str_c(text, collapse = " ")) %>%
mutate(tokens = str_count(text, " "),
type = str_split_1(text, " ") %>% unique %>% length,
ttr = type / tokens)
JOC_US_spell  %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
mutate(tokens = str_count(text, " "),
type = str_split_1(text, " ") %>% unique %>% length,
ttr = type / tokens)
JOC %>%
summarise(text = str_c(text, collapse = " ")) %>%
mutate(tokens = str_count(text, " "),
type = str_split_1(text, " ") %>% unique %>% length,
ttr = type / tokens)
JOC_US_spell  %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
mutate(tokens = str_count(text, " "),
type = str_split_1(text, " ") %>% unique %>% length,
ttr = type / tokens)
JOC_UK_spell %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
mutate(tokens = str_count(text, " "),
type = str_split_1(text, " ") %>% unique %>% length,
ttr = type / tokens)
JOC_US_spell  %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
mutate(tokens = str_count(text, " "),
type = str_split_1(text, " ") %>% unique %>% length,
ttr = type / tokens)
JOC %>%
summarise(text = str_c(text, collapse = " ")) %>%
mutate(tokens = str_count(text, " "),
type = str_split_1(text, " ") %>% unique %>% length,
ttr = type / tokens)
HOC_UK_spell %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
mutate(tokens = str_count(text, " "),
type = str_split_1(text, " ") %>% unique %>% length,
ttr = type / tokens)
HOC_US_spell  %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
mutate(tokens = str_count(text, " "),
type = str_split_1(text, " ") %>% unique %>% length,
ttr = type / tokens)
HOC %>%
summarise(text = str_c(text, collapse = " ")) %>%
mutate(tokens = str_count(text, " "),
type = str_split_1(text, " ") %>% unique %>% length,
ttr = type / tokens)
tibble(HOC_UK, HOC_US, JOC_UK, JOC_US)
HOC_UK_spell %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
unnest_tokens(tokens, text)
HOC_UK_spell %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
tidytext::unnest_tokens(tokens, text)
HOC_UK_spell %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
tidytext::unnest_tokens(tokens, text) %>%
nest()
HOC_UK_spell %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
tidytext::unnest_tokens(tokens, text) %>%
nest() %>%
mutate(tokens = map_int(data, !length(.x$tokens)))
HOC_UK_spell %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
tidytext::unnest_tokens(tokens, text) %>%
nest() %>%
mutate(tokens = map_int(data, ~length(.x$tokens)))
HOC_UK_spell %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
tidytext::unnest_tokens(tokens, text) %>%
nest() %>%
mutate(tokens = map_int(data, ~length(.x$tokens)),
types = map_int(data ~ length(unique(~.x$tokens))))
HOC_UK_spell %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
tidytext::unnest_tokens(tokens, text) %>%
nest() %>%
mutate(tokens = map_int(data, ~length(.x$tokens)),
types = map_int(data ~ length(unique(.x$tokens))))
HOC_UK_spell %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
tidytext::unnest_tokens(tokens, text) %>%
nest() %>%
mutate(tokens = map_int(data, ~length(.x$tokens)),
types = map_int(data ~ unique(.x$tokens) %>% length)))
HOC_UK_spell %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
tidytext::unnest_tokens(tokens, text) %>%
nest() %>%
mutate(tokens = map_int(data, ~length(.x$tokens)),
types = map_int(data ~ unique(.x$tokens) %>% length))
HOC_UK_spell %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
tidytext::unnest_tokens(tokens, text) %>%
nest() %>%
mutate(tokens = map_int(data, ~length(.x$tokens)),
types = map_int(data, ~ unique(.x$tokens) %>% length))
HOC_UK_spell %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
tidytext::unnest_tokens(tokens, text) %>%
nest() %>%
mutate(tokens = map_int(data, ~length(.x$tokens)),
types = map_int(data, ~ unique(.x$tokens) %>% length),
ttr = types/tokens)
### calculate ttr
HOC_ttr <-
HOC %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
tidytext::unnest_tokens(tokens, text) %>%
nest() %>%
mutate(tokens = map_int(data, ~length(.x$tokens)),
types = map_int(data, ~ unique(.x$tokens) %>% length),
ttr = types/tokens) %>%
pull(ttr)
HOC_UK_ttr <-
HOC_UK_spell %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
tidytext::unnest_tokens(tokens, text) %>%
nest() %>%
mutate(tokens = map_int(data, ~length(.x$tokens)),
types = map_int(data, ~ unique(.x$tokens) %>% length),
ttr = types/tokens) %>%
pull(ttr)
HOC_US_ttr <-
HOC_Us_spell %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
tidytext::unnest_tokens(tokens, text) %>%
nest() %>%
mutate(tokens = map_int(data, ~length(.x$tokens)),
types = map_int(data, ~ unique(.x$tokens) %>% length),
ttr = types/tokens) %>%
pull(ttr)
### calculate ttr
HOC_ttr <-
HOC %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
tidytext::unnest_tokens(tokens, text) %>%
nest() %>%
mutate(tokens = map_int(data, ~length(.x$tokens)),
types = map_int(data, ~ unique(.x$tokens) %>% length),
ttr = types/tokens) %>%
pull(ttr)
HOC_UK_ttr <-
HOC_UK_spell %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
tidytext::unnest_tokens(tokens, text) %>%
nest() %>%
mutate(tokens = map_int(data, ~length(.x$tokens)),
types = map_int(data, ~ unique(.x$tokens) %>% length),
ttr = types/tokens) %>%
pull(ttr)
HOC_US_ttr <-
HOC_US_spell %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
tidytext::unnest_tokens(tokens, text) %>%
nest() %>%
mutate(tokens = map_int(data, ~length(.x$tokens)),
types = map_int(data, ~ unique(.x$tokens) %>% length),
ttr = types/tokens) %>%
pull(ttr)
JOC_ttr <-
JOC %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
tidytext::unnest_tokens(tokens, text) %>%
nest() %>%
mutate(tokens = map_int(data, ~length(.x$tokens)),
types = map_int(data, ~ unique(.x$tokens) %>% length),
ttr = types/tokens) %>%
pull(ttr)
JOC_UK_ttr <-
JOC_UK_spell %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
tidytext::unnest_tokens(tokens, text) %>%
nest() %>%
mutate(tokens = map_int(data, ~length(.x$tokens)),
types = map_int(data, ~ unique(.x$tokens) %>% length),
ttr = types/tokens) %>%
pull(ttr)
JOC_US_ttr <-
JOC_US_spell %>%
mutate(text = str_remove(text, "\\{.+?\\}")) %>%
summarise(text = str_c(text, collapse = " ")) %>%
tidytext::unnest_tokens(tokens, text) %>%
nest() %>%
mutate(tokens = map_int(data, ~length(.x$tokens)),
types = map_int(data, ~ unique(.x$tokens) %>% length),
ttr = types/tokens) %>%
pull(ttr)
tribble(~base, ~US, ~UK,
JOC_ttr, JOC_US_ttr, JOC_UK_ttr)
tribble(_corpus, ~base, ~US, ~UK,
tribble(~corpus, ~base, ~US, ~UK,
"JOC", JOC_ttr, JOC_US_ttr, JOC_UK_ttr,
"HOC", HOC_ttr, HOC_US_ttr, HOC_UK_ttr)
mydt <- fread("/Users/timmarchand/Documents/bnc_fic_st.csv",
drop = "text")
library(data.table)
library(tidyverse)
mydt <- fread("/Users/timmarchand/Documents/bnc_fic_st.csv",
drop = "text")
mydt %>%
tibble() %>%
select(-c(doc_id,paragraph_id,sentence_id, sentence,deps,misc)) %>%
write_csv("/Users/timmarchand/Documents/bnc_fic_st.csv")
check()
library(devtools)
check()
use_r("find_non_ascii")
check()
check()
check()
library(devtools)
check()
rlang::last_trace()
check()
check()
check()
check()
check()
